\section{Introduction}

L'analyse de sentiment de courts textes devient fondamentale lorsque les perceptions sont consid\'er\'ees comme un actif informationnel critique pour les responsables de produits et de services \cite{b1}. Cela est particuli\`erement pertinent dans le d\'eveloppement de syst\`emes centr\'es sur les \'emotions, capables de fournir des informations exploitables pour am\'eliorer l'exp\'erience utilisateur ou client. Par exemple, ces informations peuvent conduire \`a des ajustements des strat\'egies de support client ou \`a des campagnes marketing plus cibl\'ees \cite{b2}. Dans ce contexte, les r\'eseaux sociaux --- et plus sp\'ecifiquement les messages courts tels que les tweets et les commentaires sur des plateformes multim\'edias --- figurent parmi les sources les plus utilis\'ees pour conduire ce type d'analyse.

Ce projet porte sur l'analyse automatique de sentiment de courts textes en anglais. Une phase exploratoire est d'abord men\'ee : pr\'etraitement du contenu, puis analyse pr\'eliminaire au moyen de m\'ethodes classiques d'apprentissage automatique. Ensuite, la classification est r\'ealis\'ee avec des classifieurs standards (Naive Bayes, r\'egression logistique et SVM lin\'eaire), en utilisant plusieurs sch\'emas de repr\'esentation (sac de mots, TF--IDF au niveau mot, et TF--IDF au niveau caract\`ere). Les performances sont rapport\'ees via l'accuracy, le Macro-F1 et des m\'etriques compl\'ementaires afin d'assurer une comparaison \'equitable.

Dans un second temps, un perceptron multicouche (MLP) entra\^{\i}n\'e sur des repr\'esentations vectoris\'ees est \'evalu\'e, et une alternative bas\'ee sur des embeddings BERT est \'etudi\'ee pour capturer des s\'emantiques contextuelles. Les performances des MLP construits pour chaque famille de repr\'esentation sont compar\'ees sur plusieurs architectures, adapt\'ees \`a la quantit\'e d'information fournie par la vectorisation (ou par BERT) et orient\'ees vers une classification \`a trois classes. La profondeur est contrainte pour limiter le sur-apprentissage, et des couches de dropout sont ins\'er\'ees entre couches cach\'ees afin de r\'egulariser l'entra\^{\i}nement.

Enfin, afin d'am\'eliorer la classification, des strat\'egies bas\'ees sur des grands mod\`eles de langage (LLM) sont \'evalu\'ees en utilisant la version API de Gemma 3-4b-it (Gemini), afin de comparer ses capacit\'es de classification \`a celles des mod\`eles entra\^{\i}n\'es. Par ailleurs, LoRA est utilis\'e pour r\'ealiser un fine-tuning efficace de transformeurs bas\'es sur BERT \cite{b3}.
