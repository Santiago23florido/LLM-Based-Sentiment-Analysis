{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ded8dd7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# Add Q2/functions to sys.path whether the notebook runs from repo root or /Q2\n",
    "cwd = Path.cwd()\n",
    "if (cwd / \"functions\").exists():\n",
    "    functions_dir = cwd / \"functions\"\n",
    "else:\n",
    "    functions_dir = cwd / \"Q2\" / \"functions\"\n",
    "\n",
    "sys.path.insert(0, str(functions_dir))\n",
    "\n",
    "from model_heads import SentimentMLPClassifier, LinearHead\n",
    "from eval_utils import evaluate_model  # detailed plots + ROC\n",
    "\n",
    "# Reproducibility\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9be3fb8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if (cwd / \"train_df_processed.csv\").exists():\n",
    "    train_path = cwd / \"train_df_processed.csv\"\n",
    "    test_path = cwd / \"test_df_processed.csv\"\n",
    "else:\n",
    "    train_path = cwd.parent / \"train_df_processed.csv\"\n",
    "    test_path = cwd.parent / \"test_df_processed.csv\"\n",
    "\n",
    "train_df = pd.read_csv(train_path)\n",
    "test_df = pd.read_csv(test_path)\n",
    "\n",
    "# Basic safety\n",
    "train_df = train_df.dropna(subset=[\"processed_text\", \"sentiment_class\"])\n",
    "test_df = test_df.dropna(subset=[\"processed_text\", \"sentiment_class\"])\n",
    "\n",
    "print(\"train_df:\", train_df.shape)\n",
    "print(\"test_df :\", test_df.shape)\n",
    "train_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71dd9924",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------- Train/Val split (stratified) --------------------\n",
    "X_text = train_df[\"processed_text\"].astype(str).values\n",
    "y = train_df[\"sentiment_class\"].astype(int).values\n",
    "\n",
    "X_train_text, X_val_text, y_train, y_val = train_test_split(\n",
    "    X_text,\n",
    "    y,\n",
    "    test_size=0.20,\n",
    "    random_state=SEED,\n",
    "    stratify=y,\n",
    ")\n",
    "\n",
    "X_test_text = test_df[\"processed_text\"].astype(str).values\n",
    "y_test = test_df[\"sentiment_class\"].astype(int).values\n",
    "\n",
    "print(\"Train size:\", len(X_train_text))\n",
    "print(\"Val size  :\", len(X_val_text))\n",
    "print(\"Test size :\", len(X_test_text))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4ca97ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================== 1) Vectorization: Frozen BERT embeddings ====================\n",
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "PRETRAINED_MODEL = \"bert-base-uncased\"\n",
    "MAX_LENGTH = 128\n",
    "BERT_BATCH_SIZE = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb7f2866",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextDataset(Dataset):\n",
    "    # Dataset that tokenizes text for BERT\n",
    "    def __init__(self, texts, labels, tokenizer, max_length):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            texts (list): List of text samples.\n",
    "            labels (list): List of sentiment labels (e.g., 0, 1).\n",
    "            tokenizer (transformers.BertTokenizer): Tokenizer for BERT.\n",
    "            max_length (int): Maximum length for tokenized sequences.\n",
    "        \"\"\"\n",
    "        self.texts = list(texts)\n",
    "        self.labels = list(labels)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Tokenize and encode the text\n",
    "        text = self.texts[idx]\n",
    "        label = int(self.labels[idx])\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            max_length=self.max_length,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        return {\n",
    "            \"input_ids\": encoding[\"input_ids\"].squeeze(0),\n",
    "            \"attention_mask\": encoding[\"attention_mask\"].squeeze(0),\n",
    "            \"label\": torch.tensor(label, dtype=torch.long),\n",
    "        }\n",
    "\n",
    "# Extract embeddings for all data\n",
    "def extract_embeddings(bert_model, dataloader, device):\n",
    "    \"\"\"\n",
    "    Extracts embeddings for all data using a pre-trained BERT model.\n",
    "\n",
    "    Args:\n",
    "        model (transformers.BertModel): Pre-trained BERT model.\n",
    "        dataloader (DataLoader): DataLoader for the dataset.\n",
    "        device (torch.device): Device to run the model on (CPU or GPU).\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: A matrix of size (number_of_samples, embedding_size).\n",
    "    \"\"\"\n",
    "    bert_model.eval()  # Set the model to evaluation mode\n",
    "    embeddings = []\n",
    "\n",
    "    with torch.no_grad():  # Disable gradient computation\n",
    "        for batch in dataloader:\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "\n",
    "            # Forward pass through BERT\n",
    "            outputs = bert_model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            pooled = outputs.pooler_output  # CLS token representation\n",
    "\n",
    "            # Append embeddings to the list\n",
    "            embeddings.append(pooled.cpu())\n",
    "            \n",
    "    # Combine all embeddings into a single matrix\n",
    "    return torch.cat(embeddings, dim=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6a7474f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------- Compute or load cached embeddings --------------------\n",
    "# Caching avoids recomputing BERT embeddings every run.\n",
    "\n",
    "out_dir = Path(\"outputs\") if (cwd / \"outputs\").exists() else (cwd / \"Q2\" / \"outputs\")\n",
    "out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "train_emb_path = out_dir / \"bert_train_embeddings.npy\"\n",
    "val_emb_path = out_dir / \"bert_val_embeddings.npy\"\n",
    "test_emb_path = out_dir / \"bert_test_embeddings.npy\"\n",
    "\n",
    "if train_emb_path.exists() and val_emb_path.exists() and test_emb_path.exists():\n",
    "    print(\"Loading cached embeddings from:\", out_dir)\n",
    "    X_train_emb = np.load(train_emb_path)\n",
    "    X_val_emb = np.load(val_emb_path)\n",
    "    X_test_emb = np.load(test_emb_path)\n",
    "else:\n",
    "    print(\"Computing BERT embeddings (this may take a while)...\")\n",
    "\n",
    "    tokenizer = BertTokenizer.from_pretrained(PRETRAINED_MODEL)\n",
    "    bert = BertModel.from_pretrained(PRETRAINED_MODEL).to(device)\n",
    "\n",
    "    train_ds = TextDataset(X_train_text, y_train, tokenizer, MAX_LENGTH)\n",
    "    val_ds = TextDataset(X_val_text, y_val, tokenizer, MAX_LENGTH)\n",
    "    test_ds = TextDataset(X_test_text, y_test, tokenizer, MAX_LENGTH)\n",
    "\n",
    "    train_loader = DataLoader(train_ds, batch_size=BERT_BATCH_SIZE, shuffle=False)\n",
    "    val_loader = DataLoader(val_ds, batch_size=BERT_BATCH_SIZE, shuffle=False)\n",
    "    test_loader = DataLoader(test_ds, batch_size=BERT_BATCH_SIZE, shuffle=False)\n",
    "\n",
    "    train_emb = extract_embeddings(bert, train_loader, device)\n",
    "    val_emb = extract_embeddings(bert, val_loader, device)\n",
    "    test_emb = extract_embeddings(bert, test_loader, device)\n",
    "\n",
    "    X_train_emb = train_emb.numpy()\n",
    "    X_val_emb = val_emb.numpy()\n",
    "    X_test_emb = test_emb.numpy()\n",
    "\n",
    "    np.save(train_emb_path, X_train_emb)\n",
    "    np.save(val_emb_path, X_val_emb)\n",
    "    np.save(test_emb_path, X_test_emb)\n",
    "\n",
    "    print(\"Saved embeddings to:\", out_dir)\n",
    "\n",
    "print(\"X_train_emb:\", X_train_emb.shape)\n",
    "print(\"X_val_emb  :\", X_val_emb.shape)\n",
    "print(\"X_test_emb :\", X_test_emb.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b782b5c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================== 2) After vectorization cleaning ====================\n",
    "# 1) Sanity checks\n",
    "assert not np.isnan(X_train_emb).any(), \"NaNs found in train embeddings\"\n",
    "assert not np.isinf(X_train_emb).any(), \"Infs found in train embeddings\"\n",
    "\n",
    "# 2) Standardization (recommended for MLP stability)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_emb)\n",
    "X_val_scaled = scaler.transform(X_val_emb)\n",
    "X_test_scaled = scaler.transform(X_test_emb)\n",
    "\n",
    "print(\"Scaled embeddings computed.\")\n",
    "print(\"Train mean (first 5 dims):\", X_train_scaled.mean(axis=0)[:5])\n",
    "print(\"Train std  (first 5 dims):\", X_train_scaled.std(axis=0)[:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c679f5a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingDataset(Dataset):\n",
    "    # Dataset that serves precomputed embeddings\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.tensor(X, dtype=torch.float32)\n",
    "        self.y = torch.tensor(y, dtype=torch.long)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            \"x\": self.X[idx],\n",
    "            \"label\": self.y[idx],\n",
    "        }\n",
    "\n",
    "# DataLoaders for the MLP\n",
    "MLP_BATCH_SIZE = 128\n",
    "\n",
    "train_loader = DataLoader(EmbeddingDataset(X_train_scaled, y_train), batch_size=MLP_BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(EmbeddingDataset(X_val_scaled, y_val), batch_size=MLP_BATCH_SIZE, shuffle=False)\n",
    "test_loader = DataLoader(EmbeddingDataset(X_test_scaled, y_test), batch_size=MLP_BATCH_SIZE, shuffle=False)\n",
    "\n",
    "print(\"MLP_BATCH_SIZE:\", MLP_BATCH_SIZE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfef19a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================== 3) Modeling ====================\n",
    "NUM_CLASSES = len(np.unique(y_train))\n",
    "INPUT_DIM = X_train_scaled.shape[1]\n",
    "\n",
    "print(\"INPUT_DIM:\", INPUT_DIM)\n",
    "print(\"NUM_CLASSES:\", NUM_CLASSES)\n",
    "\n",
    "# Define multiple MLP structures to justify/compare (same training loop)\n",
    "experiments = [\n",
    "    {\n",
    "        \"name\": \"LinearHead_baseline\",\n",
    "        \"hidden_layers\": (),\n",
    "        \"dropout\": 0.0,\n",
    "        \"activation\": \"relu\",\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"MLP_256_64_drop0.2\",\n",
    "        \"hidden_layers\": (256, 64),\n",
    "        \"dropout\": 0.2,\n",
    "        \"activation\": \"relu\",\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"MLP_128_32_drop0.2\",\n",
    "        \"hidden_layers\": (128, 32),\n",
    "        \"dropout\": 0.2,\n",
    "        \"activation\": \"relu\",\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"MLP_512_128_drop0.3\",\n",
    "        \"hidden_layers\": (512, 128),\n",
    "        \"dropout\": 0.3,\n",
    "        \"activation\": \"relu\",\n",
    "    },\n",
    "]\n",
    "\n",
    "experiments\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c7d26f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "\n",
    "def train_one_epoch(model, loader, optimizer, criterion):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for batch in loader:\n",
    "        x = batch[\"x\"].to(device)\n",
    "        yb = batch[\"label\"].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(x)\n",
    "        loss = criterion(logits, yb)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item() * x.size(0)\n",
    "\n",
    "    return running_loss / len(loader.dataset)\n",
    "\n",
    "\n",
    "def predict_proba(model, loader):\n",
    "    model.eval()\n",
    "    all_logits = []\n",
    "    all_y = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            x = batch[\"x\"].to(device)\n",
    "            yb = batch[\"label\"].cpu().numpy()\n",
    "\n",
    "            logits = model(x).cpu().numpy()\n",
    "            all_logits.append(logits)\n",
    "            all_y.append(yb)\n",
    "\n",
    "    logits = np.vstack(all_logits)\n",
    "    y_true = np.concatenate(all_y)\n",
    "\n",
    "    # Softmax -> probabilities\n",
    "    exp = np.exp(logits - logits.max(axis=1, keepdims=True))\n",
    "    probs = exp / exp.sum(axis=1, keepdims=True)\n",
    "\n",
    "    y_pred = probs.argmax(axis=1)\n",
    "    return y_true, y_pred, probs\n",
    "\n",
    "\n",
    "def macro_f1(y_true, y_pred):\n",
    "    return float(f1_score(y_true, y_pred, average=\"macro\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10d19dee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train each experiment with the same loop, select best by validation macro-F1\n",
    "\n",
    "EPOCHS = 30\n",
    "LEARNING_RATE = 1e-3\n",
    "WEIGHT_DECAY = 1e-2\n",
    "PATIENCE = 5  # early stopping patience\n",
    "\n",
    "results = []\n",
    "best_models = {}\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "for cfg in experiments:\n",
    "    name = cfg[\"name\"]\n",
    "    hidden_layers = cfg[\"hidden_layers\"]\n",
    "    dropout = cfg[\"dropout\"]\n",
    "    activation = cfg[\"activation\"]\n",
    "\n",
    "    if len(hidden_layers) == 0:\n",
    "        model = LinearHead(INPUT_DIM, NUM_CLASSES).to(device)\n",
    "    else:\n",
    "        model = SentimentMLPClassifier(\n",
    "            input_dim=INPUT_DIM,\n",
    "            num_classes=NUM_CLASSES,\n",
    "            hidden_layers=hidden_layers,\n",
    "            dropout=dropout,\n",
    "            activation=activation,\n",
    "        ).to(device)\n",
    "\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode=\"max\", patience=2, factor=0.5)\n",
    "\n",
    "    best_val_f1 = -1.0\n",
    "    best_state = None\n",
    "    epochs_no_improve = 0\n",
    "\n",
    "    history = []\n",
    "\n",
    "    print(\"\\n\" + \"=\"*100)\n",
    "    print(\"Training:\", name)\n",
    "    print(\"hidden_layers:\", hidden_layers, \"dropout:\", dropout)\n",
    "\n",
    "    for epoch in range(1, EPOCHS + 1):\n",
    "        train_loss = train_one_epoch(model, train_loader, optimizer, criterion)\n",
    "\n",
    "        yv_true, yv_pred, yv_probs = predict_proba(model, val_loader)\n",
    "        val_f1 = macro_f1(yv_true, yv_pred)\n",
    "\n",
    "        scheduler.step(val_f1)\n",
    "\n",
    "        history.append({\n",
    "            \"epoch\": epoch,\n",
    "            \"train_loss\": train_loss,\n",
    "            \"val_f1_macro\": val_f1,\n",
    "            \"lr\": optimizer.param_groups[0][\"lr\"],\n",
    "        })\n",
    "\n",
    "        print(f\"Epoch {epoch:02d} | train_loss={train_loss:.4f} | val_f1_macro={val_f1:.4f} | lr={optimizer.param_groups[0]['lr']:.2e}\")\n",
    "\n",
    "        if val_f1 > best_val_f1 + 1e-4:\n",
    "            best_val_f1 = val_f1\n",
    "            best_state = deepcopy(model.state_dict())\n",
    "            epochs_no_improve = 0\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "\n",
    "        if epochs_no_improve >= PATIENCE:\n",
    "            print(f\"Early stopping triggered (patience={PATIENCE}).\")\n",
    "            break\n",
    "\n",
    "    # Restore best model for this configuration\n",
    "    if best_state is not None:\n",
    "        model.load_state_dict(best_state)\n",
    "\n",
    "    # Final validation metrics for logging\n",
    "    yv_true, yv_pred, yv_probs = predict_proba(model, val_loader)\n",
    "\n",
    "    results.append({\n",
    "        \"model\": name,\n",
    "        \"val_f1_macro\": macro_f1(yv_true, yv_pred),\n",
    "    })\n",
    "\n",
    "    best_models[name] = model\n",
    "\n",
    "# Results table\n",
    "results_df = pd.DataFrame(results).sort_values(\"val_f1_macro\", ascending=False)\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8939c844",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================== 4) Results ====================\n",
    "# Pick the best configuration by validation macro-F1, then evaluate on test.\n",
    "\n",
    "best_name = results_df.iloc[0][\"model\"]\n",
    "print(\"Best model by val_f1_macro:\", best_name)\n",
    "\n",
    "best_model = best_models[best_name]\n",
    "\n",
    "yt_true, yt_pred, yt_probs = predict_proba(best_model, test_loader)\n",
    "\n",
    "print(\"Test macro-F1:\", macro_f1(yt_true, yt_pred))\n",
    "\n",
    "# Export full report plot (confusion matrix + ROC, etc.)\n",
    "# It writes under Q2/reports (or ./reports if running from /Q2)\n",
    "report_dir = Path(\"reports\") if (cwd / \"reports\").exists() else (cwd / \"Q2\" / \"reports\")\n",
    "report_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "evaluate_model(\n",
    "    y_true=yt_true,\n",
    "    y_pred=yt_pred,\n",
    "    y_probs=yt_probs,\n",
    "    model_name=f\"Q2_{best_name}\",\n",
    "    output_path=str(report_dir),\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
