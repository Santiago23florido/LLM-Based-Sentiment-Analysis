\section{Q6---Architecture BERT et cadre th\'eorique}

BERT (\emph{Bidirectional Encoder Representations from Transformers}) marque un changement de paradigme en apprentissage de repr\'esentations de langage. Contrairement \`a des mod\`eles unidirectionnels (p.\ ex.\ GPT) ou \`a des concat\'enations peu profondes (p.\ ex.\ ELMo), BERT pr\'eentra\^{\i}ne des repr\'esentations profondes bidirectionnelles en conditionnant simultan\'ement \`a gauche et \`a droite \`a tous les niveaux \cite{b_bert}.

\subsection{Architecture du mod\`ele}

L'architecture est un encodeur Transformer bidirectionnel, bas\'e sur Vaswani et al.\ \cite{b_att}, compos\'e d'une pile de $L$ blocs identiques.

Deux tailles sont classiquement consid\'er\'ees \cite{b_bert} :
\begin{itemize}
    \item \textbf{BERT\textsubscript{BASE} :} $L=12$, taille cach\'ee $H=768$, $A=12$ t\^etes d'attention, $\sim$110M param\`etres.
    \item \textbf{BERT\textsubscript{LARGE} :} $L=24$, $H=1024$, $A=16$, $\sim$340M param\`etres.
\end{itemize}

\begin{figure}[!ht]
  \centering
  \includegraphics[width=\linewidth]{images/Q6/bert_arch_comparison.jpg}
  \caption{Diff\'erences d'architectures de pr\'eentra\^{\i}nement. BERT utilise un Transformer bidirectionnel \cite{b_bert}.}
  \label{fig:bert_arch}
\end{figure}

Chaque bloc comprend (i) une attention multi-t\^etes et (ii) un r\'eseau feed-forward positionnel, avec connexions r\'esiduelles et normalisation de couche.

\subsubsection{Attention multi-t\^etes}
La sortie est donn\'ee par une attention \`a produit scalaire normalis\'e :
\begin{equation}
\mathrm{Attention}(Q, K, V) = \mathrm{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V,
\label{eq:attention}
\end{equation}
o\`u $d_k$ est la dimension des cl\'es, introduite comme facteur d'\'echelle.

\subsubsection{R\'eseau feed-forward et activation GELU}
Le FFN applique deux transformations lin\'eaires avec une non-lin\'earit\'e GELU :
\begin{equation}
\mathrm{FFN}(x) = \mathrm{GELU}(xW_1 + b_1)W_2 + b_2,
\label{eq:ffn}
\end{equation}
avec une dimension interm\'ediaire de 3072 pour \texttt{BERT\textsubscript{BASE}}.

\subsubsection{Connexions r\'esiduelles et normalisation}
\begin{equation}
\mathrm{Output} = \mathrm{LayerNorm}(x + \mathrm{Sublayer}(x)).
\label{eq:addnorm}
\end{equation}

\subsection{Repr\'esentation d'entr\'ee}

\subsubsection{Tokenisation WordPiece}
BERT utilise un vocabulaire WordPiece (30\,000 tokens), limitant l'OOV via des sous-mots (ex.\ ``playing'' $\rightarrow$ \texttt{play} + \texttt{\#\#ing}).

\subsubsection{Somme d'embeddings}
Chaque token est repr\'esent\'e par la somme :
\begin{equation}
\mathbf{E} = \mathbf{E}_{token} + \mathbf{E}_{segment} + \mathbf{E}_{position}.
\label{eq:embeddings}
\end{equation}

\begin{figure}[!ht]
  \centering
  \includegraphics[width=\linewidth]{images/Q6/bert_input_rep.jpg}
  \caption{BERT input representation \cite{b_bert}.}
  \label{fig:bert_input}
\end{figure}

Comme illustr\'e en Fig.~\ref{fig:bert_input}, l'embedding d'entr\'ee de chaque token combine : (i) l'identit\'e lexicale (token), (ii) l'appartenance \`a un segment (A/B), et (iii) la position (ordre). Chaque s\'equence commence par le token \texttt{[CLS]} ; l'\'etat cach\'e final associ\'e \`a \texttt{[CLS]} ($C \in \mathbb{R}^H$) est couramment utilis\'e comme repr\'esentation agr\'eg\'ee pour la classification. Le token \texttt{[SEP]} sert \`a d\'elimitation/s\'eparation des s\'equences.

\subsection{Objectifs de pr\'eentra\^{\i}nement}

BERT est pr\'eentra\^{\i}n\'e sur BooksCorpus et Wikipedia via deux t\^aches : MLM et NSP.

\subsubsection{Masked Language Model (MLM)}
15\% des tokens sont s\'electionn\'es : 80\% remplac\'es par \texttt{[MASK]}, 10\% par un token al\'eatoire, 10\% inchang\'es, afin de r\'eduire le d\'ecalage pr\'eentra\^{\i}nement/fine-tuning.

\subsubsection{Next Sentence Prediction (NSP)}
BERT pr\'edit si une phrase $B$ suit $A$ (IsNext) ou est al\'eatoire (NotNext), afin de mod\'eliser des relations inter-phrases.

\subsection{Strat\'egies d'application}

BERT peut \^etre utilis\'e en fine-tuning end-to-end ou en approche \emph{feature-based} (extraction de caract\'eristiques). La Fig.~\ref{fig:bert_table} illustre des r\'esultats en NER o\`u la concat\'enation des quatre derni\`eres couches est comp\'etitive.

\begin{figure}[!ht]
  \centering
  \includegraphics[width=0.85\linewidth]{images/Q6/bert_ner_table.jpg}
  \caption{Approche feature-based sur CoNLL-2003 NER.}
  \label{fig:bert_table}
\end{figure}
