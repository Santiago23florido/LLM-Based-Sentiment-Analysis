\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts
% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{subfig}
\usepackage{xcolor}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
\begin{document}

\title{Text Classification — Sentiment Analysis (MI201 Project 3)\\

}

\author{\IEEEauthorblockN{1\textsuperscript{st} Carlos Adrian Meneses Gamboa}
\IEEEauthorblockA{\textit{Ingénieur Degree Programme in STIC} \\
\textit{ENSTA Paris}\\
Paris, France \\
carlos.meneses@ensta-paris.fr}
\and
\IEEEauthorblockN{2\textsuperscript{nd} Jose Daniel Chacon Gomez}
\IEEEauthorblockA{\textit{Ingénieur Degree Programme in STIC} \\
\textit{ENSTA Paris}\\
Paris, France \\
jose-daniel.chacon@ensta-paris.fr}
\and
\IEEEauthorblockN{3\textsuperscript{rd} Santiago Florido Gomez}
\IEEEauthorblockA{\textit{Ingénieur Degree Programme in STIC} \\
\textit{ENSTA Paris}\\
Paris, France \\
santiago.florido@ensta-paris.fr}
}

\maketitle

\begin{abstract}
This work presents a sentiment analysis system for short English texts and compares classical machine learning methods with a transformer-based approach. 
Using standard vector representations (bag-of-words and TF-IDF), we train and evaluate several classifiers and benchmark them against a model leveraging BERT embeddings. 
Results are reported with accuracy and macro-F1, highlighting differences in performance, robustness, and computational cost. 
The study provides practical guidance on selecting an appropriate sentiment classification pipeline under typical resource constraints.

\end{abstract}

\begin{IEEEkeywords}
sentiment analysis, NLP, text classification, TF-IDF, BERT
\end{IEEEkeywords}



\section{Introduction}

Sentiment analysis of short texts becomes fundamentally important when perceptions are considered a critical information asset for product and service owners \cite{b1}. This is especially relevant in the development of emotion-driven systems, which can yield meaningful insights to improve the user or customer experience. For example, these insights can lead to adjustments in customer-support strategies or to more targeted marketing campaigns \cite{b2}. As a conceptual input for such improvements, search systems or sentiment-analysis approaches can be adapted to focus on the emotions expressed by the target population. In this context, social networks—and more specifically short messages such as tweets and comments on multimedia platforms—are among the most commonly used sources for conducting this type of analysis.

This project focuses on the automatic sentiment analysis of short English texts. First, an exploratory phase is conducted in which the dataset content is preprocessed, and a preliminary analysis of the information is performed using traditional machine-learning methods. Subsequently, the classification stage is carried out with standard classifiers such as Naive Bayes, Logistic Regression, and Linear SVM, using multiple text representation schemes, including bag-of-words, word-level TF--IDF, and character-level TF--IDF. Model performance is reported using accuracy, macro-F1, and complementary metrics to ensure a fair comparison.

Next, a multilayer perceptron (MLP) trained on vectorized text is evaluated, and an alternative based on BERT embeddings is studied to capture contextual semantics. To this end, the performance of MLP models built for each vectorization approach is compared across four network architectures, each adapted to the amount of information provided by the corresponding vectorizer or by BERT, and oriented toward a final three-class classification. In addition, an appropriate depth is defined according to the level of detail in the input representation in order to reduce overfitting on the training data. Dropout layers are also incorporated between hidden layers to further control overfitting and overtraining.

Finally, in order to improve message classification, strategies based on large language models (LLMs) were evaluated by using the API version of the Gemma 3-4b-it (Gemini) model to compare its performance as a short-text classifier against the previously trained models. In addition, LoRA was used to perform an efficient fine-tuning of BERT-based transformers \cite{b3}.




\section{Q0-Dataset analysis with different classical machine learning model}
The Sentiment Data Analysis dataset to which access is available is composed of sets of short tweets in English that are classified into three categories according to their polarity as positive, neutral, or negative in the ``sentiment'' column. Additionally, there is information related to the tweet metadata in terms of the time of day when it was published, the user's age, and their country of origin in the columns ``Age of User'' and ``country''.

It is initially proposed to perform preprocessing of the text field, starting with the removal of null values and then, using NLTK, removing stopwords, which allows obtaining the processed text column without very frequent words in English that do not have a significant semantic contribution \cite{b4}.

\subsection{Exploratory data analysis (EDA)}

An analysis of the class distribution in the training data is performed. As shown in Fig. \ref{fig:q0-eda}, although the presence of neutral-polarity data exceeds the other classes, there is no substantial imbalance in the training dataset that could be associated with bias in the classifiers to be developed.
\begin{figure}[!ht]
  \centering
  \includegraphics[width=\linewidth]{images/Q0/Distribution_of_sentiments.png}
  \caption{Distribution of sentiments in training dataset.}
  \label{fig:q0-eda}
\end{figure}

It is useful, prior to implementing some machine-learning strategies, to examine the behavior of groups of terms (bag-of-words) with respect to polarity classes, mainly from a frequency-based perspective. 

This highlights the importance of the concept of a vectorizer, which is responsible for converting collections of texts—such as the content of the ``processed text'' column—into numerical vectors. This implies obtaining a sparse representation of each document after applying the vectorizer. This process is precisely known as vectorization, and depending on the way the numerical representation is constructed in the sparse matrix, it can be classified into several types. In this work, the following approaches are considered: BoW (Bag of Words), TF--IDF, and Char TF--IDF \cite{b5}.
\begin{itemize}
\item \textbf{BoW.} Represents a document by the occurrence of words in n-grams, ignoring the positions they occupy within the document. It builds a dictionary/vocabulary from the words in the corpus, assigns an index to each term, and then counts how many times each word appears in the document, storing those counts in the document vector. In scikit-learn, this is implemented by \texttt{CountVectorizer}, which ``converts a collection of text documents to a matrix of token counts'' \cite{b5}.

\item \textbf{TF--IDF.} Starts from a principle similar to BoW in the sense that it also produces a sparse matrix representation; however, instead of relying only on raw frequency counts, it incorporates the inverse document frequency (IDF) term, which penalizes terms that appear in many documents \cite{b5}. Conceptually, the TF--IDF weight of a term $t$ in a document $d$ is computed as in Eq.~(\ref{eq:tfidf}), where $\mathrm{tf}(t,d)$ corresponds to the (raw) term frequency in $d$, and $\mathrm{idf}(t)$ assigns lower weights to terms that are widely distributed across the corpus.

\begin{equation}
\mathrm{tfidf}(t,d) = \mathrm{tf}(t,d)\times \mathrm{idf}(t).
\label{eq:tfidf}
\end{equation}

In practice (and as implemented in common libraries such as scikit-learn), a smoothed version of IDF is typically used, defined in Eq.~(\ref{eq:idf}), where $n$ is the total number of documents in the corpus and $\mathrm{df}(t)$ is the number of documents that contain the term $t$.

\begin{equation}
\mathrm{idf}(t)=\log\left(\frac{1+n}{1+\mathrm{df}(t)}\right)+1.
\label{eq:idf}
\end{equation}

Finally, after computing TF--IDF, it is common to normalize each document vector to control for differences in document length and to stabilize the scale of the features. In this work, $\ell_2$ normalization is considered, as shown in Eq.~(\ref{eq:l2norm}), where $\mathbf{v}$ denotes the TF--IDF vector of a document.

\begin{equation}
\mathbf{v}_{\mathrm{norm}}=\frac{\mathbf{v}}{\lVert \mathbf{v} \rVert_2}
=\frac{\mathbf{v}}{\sqrt{v_1^2+v_2^2+\cdots+v_n^2}}.
\label{eq:l2norm}
\end{equation}

\item \textbf{Char--IDF.} Consists of applying TF--IDF over character n-grams rather than word n-grams, which in scikit-learn can be controlled through the \texttt{analyzer} parameter of the vectorizer\cite{b6}.
\end{itemize}

After defining the vector representation schemes for the documents, it is proposed to use BoW to extract the 20 most frequent unigrams and bigrams within the training set. In the case of TF--IDF, the objective is to display the 20 unigrams and bigrams with the highest weights in the training dataset. The use of character n-grams is not proposed for this step because, by not forming complete words, the resulting features are less intelligible for the intended analysis.



\begin{figure}[!ht]
  \centering
  \subfloat[Monograms]{
    \includegraphics[width=0.9\linewidth]{images/Q0/top20monogramsBoW.png}
    \label{fig:q0-bow-uni}}
  \hfill
  \subfloat[Bigrams]{
    \includegraphics[width=0.9\linewidth]{images/Q0/top20bigramsBoW.png}
    \label{fig:q0-bow-bi}}
  \caption{Top 20 n-gramas with BoW.}
  \label{fig:q0-bow}
\end{figure}

\begin{figure}[!ht]
  \centering
  \subfloat[Monograms]{
    \includegraphics[width=0.9\linewidth]{images/Q0/top20monogramstidf.png}
    \label{fig:q0-tfidf-uni}}
  \hfill
  \subfloat[Bigrams]{
    \includegraphics[width=0.9\linewidth]{images/Q0/top20bigramstidf.png}

    \label{fig:q0-tfidf-bi}}
  \caption{Top 20 n-gramas with TF--IDF.}
  \label{fig:q0-tfidf}
\end{figure}

By analyzing the results shown in Figs. \ref{fig:q0-bow} - \ref{fig:q0-tfidf}, it can be observed that some n-grams appearing under both methods correspond to words that, in everyday language use, are commonly associated with the polarity class in which they become most representative. This is the case for the unigram \emph{bad} or \emph{sorry} within the negative polarity class, which appears among the top terms for both vectorization methods. Nevertheless, it is evident that, since these are purely statistical approaches, words that were not removed during preprocessing but are very frequent in English usage—such as \emph{it}—may also appear. Similarly, certain bigrams that a speaker might classify as neutral, but that become frequent due to the data-collection context (e.g., \emph{feels like}), appear among the most representative n-grams across multiple classes. This anticipates one of the effects addressed in later stages: training models using frequency-based vectorizations, rather than embeddings that incorporate semantic information into the sparse vector representation of the documents.

In addition to the processed text column, which is the main focus of this work, it is of interest to determine whether some of the other dataset columns exhibit any relationship with sentiment classification. To this end, the class distributions are analyzed as a function of the user's age and the time of day at which the tweet is published, as shown in Fig. \ref{fig:q0-sbuubtt} However, the class distribution for these categorical variables remains very similar across their respective domains in both cases; therefore, they are not considered relevant for training the sentiment classification model.

\begin{figure}[!ht]

  \centering
  \subfloat[Sentiments by User]{
    \includegraphics[width=0.9\linewidth]{images/Q0/sentimentsbyuser.png}
    \label{fig:q0-sbu}}
  \hfill
  \subfloat[Sentiments by type of tweet]{
    \includegraphics[width=0.9\linewidth]{images/Q0/sentimentbytypeoftweet.png}
    \label{fig:q0-sbtt}}
  \caption{Comparative sentiment distribution by category, stratified by user and tweet type}
  \label{fig:q0-sbuubtt}
\end{figure}

In the case of the country in which the tweet is published, a distribution of the countries with the highest number of posts is analyzed in Fig. \ref{fig:q0-dcountry}. It is observed that the tweet counts do not vary in a markedly representative way among them, and although the class distributions are slightly different across countries, this feature is not included in later stages. One reason is that the model may ``memorize'' country-specific patterns that do not hold outside the dataset or that change over time. Additionally, given the shape of the country distribution, many countries have only a small number of examples, which generates rare features and increases the probability of overfitting. For these reasons, this column is ultimately not included in the analysis.


\begin{figure}[!ht]
  \centering
  \includegraphics[width=\linewidth]{images/Q0/sentimentbycountry.png}
  \caption{Distribution of sentiments in the top 10 countries.}
  \label{fig:q0-dcountry}
\end{figure}

\begin{figure}[!ht]
  \centering
  \includegraphics[width=\linewidth]{images/Q0/wordcountsentiment.png}
  \caption{Word count distribution by sentiment}
  \label{fig:q0-wordcount}
\end{figure}

Finally, as a last analysis of the dataset, it is proposed to examine whether the length of the preprocessed text has any relationship with the class, and thus whether it could be worth including it as a variable during the training of the classification models. However, as shown in Fig. \ref{fig:q0-wordcount}, the distribution of the number of words per message for each class is comparable (or effectively equivalent). For this reason, this variable is also not considered relevant for the objective of the models constructed in the next stage.

\subsection{Classic model training}

After the analysis and description of the dataset presented in the previous section, it is proposed to implement traditional machine-learning models that allow the classification of received and processed text messages into each of the three polarity (sentiment) categories. Within this work, the concept of classic machine learning models refers to supervised approaches that are not deep learning and do not rely on transformers or embeddings; instead, they operate on an explicit text representation obtained from the effect of one of the previously mentioned vectorizers. This enables following the workflow: document representation, classifier training, evaluation \cite{b7}.

Within this framework, and considering the variability associated with each of the vectorizers described previously, it is proposed that, for each text representation, a set of classifiers is trained using classic machine-learning algorithms: Multinomial Naive Bayes, Logistic Regression, Support Vector Machine, and Random Forest. However, given the variability of the hyperparameters associated with each classifier and with the vectorizer itself, a hyperparameter search is performed over a pipeline composed of the vectorizer and the selected model, and evaluated via stratified cross-validation. The approach compares the performance obtained for each candidate hyperparameter configuration across the cross-validation folds, and selects the configuration that maximizes a chosen metric. Finally, the best-performing pipeline is refit on the full training set using the selected hyperparameter setting.

In order to adequately describe the operation of the training algorithm that is employed, it is first proposed to detail the influence of each of the hyperparameters considered for every element of the pipeline, both in the case of the vectorizers and in the case of the trained models.

It is convenient to begin with the hyperparameters associated with the vectorizer within the pipeline. The first of these is \texttt{ngram\_range}, which is a parameter defined as an array of tuples indicating the size of the n-grams that are considered in the construction of the vector representation. These n-grams correspond to words in the case of BoW, to word n-grams in the case of word-level TF--IDF, and to character n-grams in the case of character-level TF--IDF. The \texttt{min\_df} parameter, in turn, contains a set of minimum thresholds to be evaluated for the document-frequency criterion, that is, the minimum number of documents in which an n-gram must appear in order to be considered. This is useful for removing very rare terms from the representation. The \texttt{max\_df} parameter specifies the maximum document frequency an n-gram can have to be included in the vectorization, which is useful for removing n-grams that are too frequent and therefore provide limited discriminative information. Finally, \texttt{max\_features} contains a set of candidate values, where each value corresponds to a limit on the number of features that is tested during the hyperparameter-search phase in order to constrain the vocabulary size, mainly as a function of frequency and computational cost. This limitation is particularly important to adapt the training of the neural networks that are preceded by character-level vectorization in Question Q2, as will be detailed later.

On the other hand, the hyperparameters associated with each model are specific to each algorithm, depending on its nature and mathematical formulation, and they are described below:

\begin{itemize}
\item \textbf{Multinomial Naive Bayes} \texttt{alpha} is an additive smoothing parameter whose main objective is to avoid zero probabilities in the case where a feature does not appear in the training data. Recall that Naive Bayes is formulated as a product of conditional probabilities under an independence assumption; in this context, smoothing works by artificially increasing the observed counts by the value of $\alpha$ \cite{b8}.

\item \textbf{Logistic Regression} \texttt{C} is a parameter that represents the inverse of the regularization strength: smaller values imply stronger regularization and, therefore, a lower risk of overfitting, whereas larger values imply weaker regularization and a model with more freedom in its parameters. The \texttt{penalty} option defines the type of regularization applied by the model: $L1$ penalizes the sum of absolute values of the coefficients, whereas $L2$ penalizes the sum of squared coefficients, which tends to shrink weights more smoothly and does not explicitly encourage sparse solutions where some coefficients become exactly zero. The \texttt{solver} parameter refers to the optimization algorithm used during training, and \texttt{max\_iter} indicates the maximum number of iterations allowed for the solver to converge \cite{b9}.

\item \textbf{Support vector machine} The \texttt{C} parameter acts as the inverse of the regularization strength. The \texttt{estimator\_\_loss} hyperparameter is used to define the loss function of the linear SVM. A \texttt{max\_iter} parameter is also included to set the maximum number of iterations allowed for convergence\cite{b10}. In addition, the SVM is wrapped within \texttt{CalibratedClassifierCV}, which converts the output of the Support Vector Machine into class probabilities, making the predictions more interpretable and, therefore, more suitable for the pipeline hyperparameter-tuning procedure \cite{b11}.

\item \textbf{Random forest} \texttt{n\_estimators} specifies the number of trees (estimators) that are trained in the forest. The \texttt{max\_depth} parameter defines the maximum depth of each tree; when it is set to \texttt{None}, the tree is allowed to grow until its leaves satisfy the stopping requirement defined by \texttt{min\_samples\_leaf}. In contrast, when a finite value is used, the number of levels in the tree is restricted, reducing the number of nodes and acting as a regularization mechanism that can decrease the risk of overfitting. The \texttt{min\_samples\_split} parameter sets the minimum number of samples required to split an internal node; it can also be used to control overfitting, and therefore it may have a regularization effect as well. Finally, \texttt{class\_weight} is used to define class weights during training: \texttt{None} uses equal weights, while \texttt{balanced} adjusts weights as a function of the class frequencies in the training data in order to counteract class imbalance \cite{b12}.

\end{itemize}

With the set of defined parameters and a specific pipeline composed of a vectorizer and a supervised learning model, a parameter grid is constructed. From this grid, the candidate combinations are enumerated and the performance of each combination is estimated via 5-fold cross-validation, aggregating the metrics across folds in order to obtain a more robust representation of the estimated model. Once the grid-search algorithm identifies an optimal configuration, the selected pipeline is retrained on the full training set. It is worth emphasizing that this design also includes the possibility of enabling or disabling parallelism in each training run, depending on the specific requirements of each execution of the training algorithm.

After concluding the analysis of the method, the training of all pipelines composed of the referenced vectorizers and models is proposed by using the hyperparameter search algorithm, and the results of this process are discussed in the following section.

\section{Prepare Your Paper Before Styling}
Before you begin to format your paper, first write and save the content as a 
separate text file. Complete all content and organizational editing before 
formatting. Please note sections \ref{AA}--\ref{SCM} below for more information on 
proofreading, spelling and grammar.

Keep your text and graphic files separate until after the text has been 
formatted and styled. Do not number text heads---{\LaTeX} will do that 
for you.

\subsection{Abbreviations and Acronyms}\label{AA}
Define abbreviations and acronyms the first time they are used in the text, 
even after they have been defined in the abstract. Abbreviations such as 
IEEE, SI, MKS, CGS, ac, dc, and rms do not have to be defined. Do not use 
abbreviations in the title or heads unless they are unavoidable.

\subsection{Units}
\begin{itemize}
\item Use either SI (MKS) or CGS as primary units. (SI units are encouraged.) English units may be used as secondary units (in parentheses). An exception would be the use of English units as identifiers in trade, such as ``3.5-inch disk drive''.
\item Avoid combining SI and CGS units, such as current in amperes and magnetic field in oersteds. This often leads to confusion because equations do not balance dimensionally. If you must use mixed units, clearly state the units for each quantity that you use in an equation.
\item Do not mix complete spellings and abbreviations of units: ``Wb/m\textsuperscript{2}'' or ``webers per square meter'', not ``webers/m\textsuperscript{2}''. Spell out units when they appear in text: ``. . . a few henries'', not ``. . . a few H''.
\item Use a zero before decimal points: ``0.25'', not ``.25''. Use ``cm\textsuperscript{3}'', not ``cc''.)
\end{itemize}

\subsection{Equations}
Number equations consecutively. To make your 
equations more compact, you may use the solidus (~/~), the exp function, or 
appropriate exponents. Italicize Roman symbols for quantities and variables, 
but not Greek symbols. Use a long dash rather than a hyphen for a minus 
sign. Punctuate equations with commas or periods when they are part of a 
sentence, as in:
\begin{equation}
a+b=\gamma\label{eq}
\end{equation}

Be sure that the 
symbols in your equation have been defined before or immediately following 
the equation. Use ``\eqref{eq}'', not ``Eq.~\eqref{eq}'' or ``equation \eqref{eq}'', except at 
the beginning of a sentence: ``Equation \eqref{eq} is . . .''

\subsection{\LaTeX-Specific Advice}

Please use ``soft'' (e.g., \verb|\eqref{Eq}|) cross references instead
of ``hard'' references (e.g., \verb|(1)|). That will make it possible
to combine sections, add equations, or change the order of figures or
citations without having to go through the file line by line.

Please don't use the \verb|{eqnarray}| equation environment. Use
\verb|{align}| or \verb|{IEEEeqnarray}| instead. The \verb|{eqnarray}|
environment leaves unsightly spaces around relation symbols.

Please note that the \verb|{subequations}| environment in {\LaTeX}
will increment the main equation counter even when there are no
equation numbers displayed. If you forget that, you might write an
article in which the equation numbers skip from (17) to (20), causing
the copy editors to wonder if you've discovered a new method of
counting.

{\BibTeX} does not work by magic. It doesn't get the bibliographic
data from thin air but from .bib files. If you use {\BibTeX} to produce a
bibliography you must send the .bib files. 

{\LaTeX} can't read your mind. If you assign the same label to a
subsubsection and a table, you might find that Table I has been cross
referenced as Table IV-B3. 

{\LaTeX} does not have precognitive abilities. If you put a
\verb|\label| command before the command that updates the counter it's
supposed to be using, the label will pick up the last counter to be
cross referenced instead. In particular, a \verb|\label| command
should not go before the caption of a figure or a table.

Do not use \verb|\nonumber| inside the \verb|{array}| environment. It
will not stop equation numbers inside \verb|{array}| (there won't be
any anyway) and it might stop a wanted equation number in the
surrounding equation.

\subsection{Some Common Mistakes}\label{SCM}
\begin{itemize}
\item The word ``data'' is plural, not singular.
\item The subscript for the permeability of vacuum $\mu_{0}$, and other common scientific constants, is zero with subscript formatting, not a lowercase letter ``o''.
\item In American English, commas, semicolons, periods, question and exclamation marks are located within quotation marks only when a complete thought or name is cited, such as a title or full quotation. When quotation marks are used, instead of a bold or italic typeface, to highlight a word or phrase, punctuation should appear outside of the quotation marks. A parenthetical phrase or statement at the end of a sentence is punctuated outside of the closing parenthesis (like this). (A parenthetical sentence is punctuated within the parentheses.)
\item A graph within a graph is an ``inset'', not an ``insert''. The word alternatively is preferred to the word ``alternately'' (unless you really mean something that alternates).
\item Do not use the word ``essentially'' to mean ``approximately'' or ``effectively''.
\item In your paper title, if the words ``that uses'' can accurately replace the word ``using'', capitalize the ``u''; if not, keep using lower-cased.
\item Be aware of the different meanings of the homophones ``affect'' and ``effect'', ``complement'' and ``compliment'', ``discreet'' and ``discrete'', ``principal'' and ``principle''.
\item Do not confuse ``imply'' and ``infer''.
\item The prefix ``non'' is not a word; it should be joined to the word it modifies, usually without a hyphen.
\item There is no period after the ``et'' in the Latin abbreviation ``et al.''.
\item The abbreviation ``i.e.'' means ``that is'', and the abbreviation ``e.g.'' means ``for example''.
\end{itemize}
An excellent style manual for science writers is \cite{b7}.

\subsection{Authors and Affiliations}
\textbf{The class file is designed for, but not limited to, six authors.} A 
minimum of one author is required for all conference articles. Author names 
should be listed starting from left to right and then moving down to the 
next line. This is the author sequence that will be used in future citations 
and by indexing services. Names should not be listed in columns nor group by 
affiliation. Please keep your affiliations as succinct as possible (for 
example, do not differentiate among departments of the same organization).

\subsection{Identify the Headings}
Headings, or heads, are organizational devices that guide the reader through 
your paper. There are two types: component heads and text heads.

Component heads identify the different components of your paper and are not 
topically subordinate to each other. Examples include Acknowledgments and 
References and, for these, the correct style to use is ``Heading 5''. Use 
``figure caption'' for your Figure captions, and ``table head'' for your 
table title. Run-in heads, such as ``Abstract'', will require you to apply a 
style (in this case, italic) in addition to the style provided by the drop 
down menu to differentiate the head from the text.

Text heads organize the topics on a relational, hierarchical basis. For 
example, the paper title is the primary text head because all subsequent 
material relates and elaborates on this one topic. If there are two or more 
sub-topics, the next level head (uppercase Roman numerals) should be used 
and, conversely, if there are not at least two sub-topics, then no subheads 
should be introduced.

\subsection{Figures and Tables}
\paragraph{Positioning Figures and Tables} Place figures and tables at the top and 
bottom of columns. Avoid placing them in the middle of columns. Large 
figures and tables may span across both columns. Figure captions should be 
below the figures; table heads should appear above the tables. Insert 
figures and tables after they are cited in the text. Use the abbreviation 
``Fig.~\ref{fig}'', even at the beginning of a sentence.


Figure Labels: Use 8 point Times New Roman for Figure labels. Use words 
rather than symbols or abbreviations when writing Figure axis labels to 
avoid confusing the reader. As an example, write the quantity 
``Magnetization'', or ``Magnetization, M'', not just ``M''. If including 
units in the label, present them within parentheses. Do not label axes only 
with units. In the example, write ``Magnetization (A/m)'' or ``Magnetization 
\{A[m(1)]\}'', not just ``A/m''. Do not label axes with a ratio of 
quantities and units. For example, write ``Temperature (K)'', not 
``Temperature/K''.

\section*{Acknowledgment}

The preferred spelling of the word ``acknowledgment'' in America is without 
an ``e'' after the ``g''. Avoid the stilted expression ``one of us (R. B. 
G.) thanks $\ldots$''. Instead, try ``R. B. G. thanks$\ldots$''. Put sponsor 
acknowledgments in the unnumbered footnote on the first page.

\section*{References}

Please number citations consecutively within brackets \cite{b1}. The 
sentence punctuation follows the bracket \cite{b2}. Refer simply to the reference 
number, as in \cite{b3}---do not use ``Ref. \cite{b3}'' or ``reference \cite{b3}'' except at 
the beginning of a sentence: ``Reference \cite{b3} was the first $\ldots$''

Number footnotes separately in superscripts. Place the actual footnote at 
the bottom of the column in which it was cited. Do not put footnotes in the 
abstract or reference list. Use letters for table footnotes.

Unless there are six authors or more give all authors' names; do not use 
``et al.''. Papers that have not been published, even if they have been 
submitted for publication, should be cited as ``unpublished'' \cite{b4}. Papers 
that have been accepted for publication should be cited as ``in press'' \cite{b5}. 
Capitalize only the first word in a paper title, except for proper nouns and 
element symbols.

For papers published in translation journals, please give the English 
citation first, followed by the original foreign-language citation \cite{b6}.

\begin{thebibliography}{00}

\bibitem{b1} T. Finn and A. Downie, ``How can sentiment analysis be used to improve customer experience?,'' \emph{IBM Think}, accessed Jan. 18, 2026. [Online]. Available: https://www.ibm.com/think/insights/how-can-sentiment-analysis-be-used-to-improve-customer-experience

\bibitem{b2} B. Pang and L. Lee, ``Opinion mining and sentiment analysis,'' \emph{Foundations and Trends in Information Retrieval}, vol. 2, no. 1--2, pp. 1--135, 2008.

\bibitem{b3} E. J. Hu, Y. Shen, P. Wallis, Z. Allen-Zhu, Y. Li, S. Wang, L. Wang, and W. Chen, ``LoRA: Low-Rank Adaptation of Large Language Models,'' \emph{arXiv preprint} arXiv:2106.09685, Jun. 2021, doi: 10.48550/arXiv.2106.09685.

\bibitem{b4} The NLTK Project, ``Sample usage for corpus,'' \emph{NLTK Documentation}, accessed Jan. 18, 2026. [Online]. Available: https://www.nltk.org/howto/corpus.html

\bibitem{b5} scikit-learn developers, ``Feature extraction,'' \emph{scikit-learn Documentation}, accessed Jan. 18, 2026. [Online]. Available: https://scikit-learn.org/stable/modules/feature\_extraction.html

\bibitem{b6} scikit-learn developers, ``sklearn.feature\_extraction.text.TfidfVectorizer,'' \emph{scikit-learn Documentation}, accessed Jan. 18, 2026. [Online]. Available: https://scikit-learn.org/stable/modules/generated/sklearn.feature\_extraction.text.TfidfVectorizer.html

\bibitem{b7} F. Sebastiani, ``Machine Learning in Automated Text Categorization,'' \emph{ACM Computing Surveys}, vol. 34, no. 1, pp. 1--47, Mar. 2002, doi: 10.1145/505282.505283. [Online]. Available: https://nmis.isti.cnr.it/sebastiani/Publications/ACMCS02.pdf

\bibitem{b8} scikit-learn developers, ``sklearn.naive\_bayes.MultinomialNB,'' \emph{scikit-learn Documentation}. [Online]. Available: https://scikit-learn.org/stable/modules/generated/sklearn.naive\_bayes.MultinomialNB.html. Accessed: Jan. 18, 2026.

\bibitem{b9} scikit-learn developers, ``sklearn.linear\_model.LogisticRegression,'' \emph{scikit-learn Documentation}. [Online]. Available: https://scikit-learn.org/stable/modules/generated/sklearn.linear\_model.LogisticRegression.html. Accessed: Jan. 18, 2026.

\bibitem{b10} scikit-learn developers, ``sklearn.svm.LinearSVC,'' \emph{scikit-learn Documentation}, accessed Jan. 18, 2026. [Online]. Available: https://scikit-learn.org/stable/modules/generated/sklearn.svm.LinearSVC.html

\bibitem{b11} scikit-learn developers, ``sklearn.calibration.CalibratedClassifierCV,'' \emph{scikit-learn Documentation}, accessed Jan. 18, 2026. [Online]. Available: https://scikit-learn.org/stable/modules/generated/sklearn.calibration.CalibratedClassifierCV.html

\bibitem{b12} scikit-learn developers, ``sklearn.ensemble.RandomForestClassifier,'' \emph{scikit-learn Documentation}, accessed Jan. 18, 2026. [Online]. Available: https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html

\end{thebibliography}
\vspace{12pt}
\color{red}
IEEE conference templates contain guidance text for composing and formatting conference papers. Please ensure that all template text is removed from your conference paper prior to submission to the conference. Failure to remove the template text from your paper may result in your paper not being published.

\end{document}
